<h1>BAP_NAL</h1>
<h2>基础信息</h2>
<p>文章标题：Background-Aware Pooling and Noise-Aware Loss forWeakly-Supervised Semantic Segmentation</p>
<p>文章链接：</p>
<p>发表时间：</p>
<h2>背景</h2>
<h2>创新点简介</h2>
<p>①提出了BAP（背景感知池）方法，能够在边界框内分辨出前景和背景，相比于GAP方法，不会只考虑到局部。
②我们引入了一个噪音感知损失（NAL）来训练CNN的语义分割。</p>
<p><strong>效果</strong>：
在PASCAL VOC 2012数据集上使用DeepLab-V1（VGG-16）与最先进的方法进行定量比较，以mIoU计。粗体数字表示最好的性能，下划线的数字是第二好的。
<code>Image-level labels (10K) with Saliency (3K)</code></p>
<table>
<thead>
<tr>
<th>方法</th>
<th>价值</th>
<th>测试</th>
</tr>
</thead>
<tbody>
<tr>
<td>SeeNetNIPS’18</td>
<td>61.1</td>
<td>60.7</td>
</tr>
<tr>
<td>FickleNetCVPR’19</td>
<td>61.2</td>
<td>61.9</td>
</tr>
<tr>
<td>OAAICCV’19</td>
<td>63.1</td>
<td>62.8</td>
</tr>
<tr>
<td>ICDCVPR’20</td>
<td>64.0</td>
<td>63.9</td>
</tr>
</tbody>
</table>
<p><code>Supervision: Boxes (10K)</code></p>
<table>
<thead>
<tr>
<th>方法</th>
<th>价值</th>
<th>测试</th>
</tr>
</thead>
<tbody>
<tr>
<td>BoxSupICCV’15</td>
<td>62.0</td>
<td>64.6</td>
</tr>
<tr>
<td>WSSLICCV’15</td>
<td>60.6</td>
<td>62.2</td>
</tr>
<tr>
<td>SDICVPR’17</td>
<td>65.7</td>
<td>67.5</td>
</tr>
<tr>
<td>BCMCVPR’19</td>
<td>66.8</td>
<td>-</td>
</tr>
<tr>
<td>w/Y_{crf}</td>
<td>67.8</td>
<td>-</td>
</tr>
<tr>
<td>w/Y_{ret}</td>
<td>66.1</td>
<td>-</td>
</tr>
<tr>
<td>w/Y_NAL</td>
<td>68.1</td>
<td>69.4</td>
</tr>
</tbody>
</table>
<p><code>Boxes (9K) with Masks (1K)</code></p>
<table>
<thead>
<tr>
<th>方法</th>
<th>价值</th>
<th>测试</th>
</tr>
</thead>
<tbody>
<tr>
<td>BoxSupICCV’15</td>
<td>63.5</td>
<td>66.2</td>
</tr>
<tr>
<td>WSSLICCV’15</td>
<td>65.1</td>
<td>66.6</td>
</tr>
<tr>
<td>SDICVPR’17</td>
<td>65.8</td>
<td>66.9</td>
</tr>
<tr>
<td>BCMCVPR’19</td>
<td>67.5</td>
<td>-</td>
</tr>
<tr>
<td>w/ NAL</td>
<td>70.5</td>
<td>71.5</td>
</tr>
</tbody>
</table>
<h2>详细内容</h2>
<h3>BAP部分</h3>
<p>【整体流程】如下图所示，输入通过特征提取，接下来将特征值池化成N*N的格子，通过softmax，分类每一个类（包括背景）进行训练，使网络可以对目标进行提取。</p>
<p><img src="../../../img/article/2021-11-03-21-39-58.png" alt=""></p>
<p>【背景平均特征向量】在这个过程中，为了得到前景（目标）的伪标签，作者逆向思维从背景出发进行寻找（找到背景剩下的就是前景了）。首先通过公式（1）来计算一小块N*N格子中背景的均值向量，其中G(j)表示每个网格单元，M(p)不属于 前景框的像素。通过公式（1）的计算，就使得 <em>qj</em> 的值表示了N x N格子中的背景的所有像素的特征向量均值。</p>
<p><img src="../../../img/article/2021-11-03-23-43-54.png" alt=""></p>
<p><img src="../../../img/article/2021-11-04-00-03-26.png" alt=""></p>
<p>【背景概率图】接着我们用这j个特征向量均值和前景框里的每一个像素上的特征向量进行一次<text color="red">余弦相似度计算</text>，即公式（3）。如果一个格子全是背景，其特征向量经过均值处理，还是表示背景，在和背景上的像素做余弦相似度时，其值会十分接近于1，而对于在前景的像素格子，前景像素点的特征向量和背景均值的特征向量差异较大，其值接近会较小（可以类比将3个指向12点的向量，进行平均，那么他还是12点方向），这样就可以近似的区分前景和背景了。对于每一个 <em>qj</em> 和前景框里像素做余弦相似度，都会产生一张余弦相似度图，一共有j个，为了综合表示前景和背景的概率，作者对这j个余弦相似度图取了平均，即公式（2）。（有人可能会说，背景也有变化很大的情况呀，进行均值以后再和每一个背景像素进行比较，能一样吗？如果你是这么想的，那么这里你忽略了一个问题，我们现在说的背景已经是特征图了，经过卷积的特征图包含的背景信息是相交统一的，具体可以参考CAM操作）</p>
<p><img src="../../../img/article/2021-11-03-23-58-12.png" alt=""></p>
<p><img src="../../../img/article/2021-11-03-23-58-22.png" alt=""></p>
<p><strong>注：f(p)/||f(p)|| 和 qj/||qj||都表示向量归一化，B表示前景框里的像素集合</strong></p>
<p><img src="../../../img/article/2021-11-04-10-17-47.png" alt=""></p>
<p>【前景概率图】求出了背景的区域的概率A(p)，这样，我们用1减去这个A(p)就得到了每一个像素属于目标物体的概率。接着，类似于公式（1），这里的公式（4），其中 <em>ri</em> 表示了一个前景格子中前景部分的平均特征向量。</p>
<p><img src="../../../img/article/2021-11-04-10-54-14.png" alt=""></p>
<p><img src="../../../img/article/2021-11-04-11-06-12.png" alt=""></p>
<p>【训练过程】
这里的qj和ri可以理解为将原图像素进行了N*N的特征池化操作，所以直接把这两个均值向量融合，当作一张特征图，后面就接图像分类的softmax那一套东西了。</p>
<p><img src="../../../img/article/2021-11-04-11-10-03.png" alt=""></p>
<h3>伪标签生产</h3>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Y</mi><mrow><mi>c</mi><mi>r</mi><mi>f</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Y_{crf}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>输入背景图片概率和经过归一处理的前景概率们，进行DenseCRF产生[]</p>
<p><img src="../../../img/article/2021-11-04-11-13-55.png" alt=""></p>
<h2>引用</h2>
