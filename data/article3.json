{"articles":[{"title":"Can Vision Transformers Learn without Natural Images","img":"","href":"ai/ai-9/0","des":"# Vision Transformers Learn without Natural Images\r\n\r\n\r\n## 摘要\r\n\r\n【函数驱动的监督学习】使用了 FDSL,(Formula-Driven Supervised Learning)函数驱动的监督学习。先前的FDSL主要是产生了形状不同的形状的图形的物体，进行训练。在本文中作者又引入了颜色和斑块来进行训练。\r\n\r\n","commend":0,"watch":0,"evaluate":0,"date":"2022-03-26T11:32:39.209Z"},{"title":"introduction","img":"","href":"ai/ai-10/0","des":"# 图卷积神经网络\r\n## 意义\r\n使用神经网络来表达一张图上的信息。图相较于其他的数据结构，存在更加明显的结构特性。一张图的信息包含有4个方面，顶点的信息，边的信息，图整体的信息，图的连接信息。如今，大部分GNN在做的事情就是以一张图片的这些信息作为输入，得到一张输出图，输出图的结构信息与原图一样，但是顶点信息，边信息，整张图的信息表达会发生改变。\r\n\r\n## 信息的表达\r\n对于顶点信息，我们可以使用一个向量来进行表示。\r\n\r\n对于每一条边，我们同样可以使用一个向量来表示。\r\n对于全局信息，我们可以使用所有点的均值与所有边的均值进行表示，也可以使用一个和全部节点相连接的伪节点进行信息的表达。\r\n对于连接性，我们可以使用邻接表或者邻接矩阵来表示。\r\n\r\n## 案例说明\r\n【顶点分类问题】：已知一张图上有若干节点，需要对这些节点进行分类。\r\n① 最简单的方式，可能我们已经有了节点的向量表达，所以只需要对每个节点做一次全连接+softmax之类的分类网络就可以表示信息了。\r\n② 信息转化，假设顶点没有合理的向量表达，或者表达能力较弱，我们可以用边的","commend":0,"watch":0,"evaluate":0,"date":"2022-03-26T11:32:39.209Z"},{"title":"PrototypicalNet","img":"","href":"ai/ai-11/0","des":"# PrototypicalNet\r\n\r\n## 信息\r\n\r\n文章标题：Prototypical Networks for Few-shot Learning\r\n\r\n文章链接：[https://arxiv.org/abs/1703.05175](https://arxiv.org/abs/1703.05175)\r\n\r\n发表时间：2017-03\r\n\r\n\r\n## 背景\r\n【度量学习】\r\n\r\n是一种学习两个样本（特征）之间相似性的学习方法。常见的度量有欧式距离、等。以上所提到的度量方法都是不可学习的，度量学习则使用神经网络来训练一个度量函数，这样做可以**对长度不同的片段进行比较**，也可以通过维度扩展的方式，来**寻找更深层次的相似性关系**。\r\n\r\n【度量空间】\r\n\r\n是指度量函数的集合\r\n\r\n【小样本学习】\r\n\r\n通过已经训练好的模型，在其基础之上加入新的分类，并且只有少量样本。在次基础上进行训练，使网络可以泛化到这些样本上。由于样本量很少，在新的类上很容易出先<font color=\"red\">过拟合的现象。</font>\r\n\r\n## 创新点简介\r\n本文使用原型网络`Prototypi","commend":0,"watch":0,"evaluate":0,"date":"2022-03-26T11:32:39.209Z"},{"title":"SETR","img":"../../../img/article/2022-03-11-14-59-06.png","href":"ai/ai-7/0","des":"# SETR\r\n\r\n## 信息\r\n\r\n论文题目：Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers\r\n\r\n文章链接：[https://arxiv.org/abs/2012.15840](https://arxiv.org/abs/2012.15840)\r\n\r\n发表时间：2020-12 \r\n\r\n## 创新点简介\r\nSETR使用transformer设计了一个端到端的语义分割网络，首先将原图切割为若干 16x16 个窗口，把其中的像素进行线性映射，得到一维编码，然后使用24层transformer的编码器来完成对于图像特征的提取，然后使用卷积做上采样操作，得到最终结果。\r\n\r\n![](../../../img/article/2022-03-11-14-59-06.png)\r\n\r\n## 优点\r\n是语义分割领域的一次创行，将transformer引入到了语义分割领域中。\r\n\r\n## 存在的问题\r\n切割的窗口过大，语义信息不仅准。\r\n","commend":0,"watch":0,"evaluate":0,"date":"2022-03-26T11:32:39.208Z"},{"title":"基础","img":"","href":"ai/ai-8/0","des":"# 目标检测算法\r\n目标检测算法，检测出一张图片上的目标物体，并通过矩形框进行标注。\r\n\r\n\r\n## 分类\r\n【是否有Anchor Box先验框】<br/>\r\n\r\n按照是否有Anchor Box先验框，可以分为Anchor-Base模型和Anchor-free模型，Anchor的提出，旨在通过先验信息，初步给出一个目标框的样子，便于模型回归。\r\nAnchor-Base的代表作是Faster-RCNN, SSD, YOLO-V2, YOLOV3;\r\nAnchor-free的代表作是YOLO-V1, FCOS\r\n\r\n【目标框选择和目标分类是否分离】<br/>\r\n\r\n按照目标框选择和目标分类是否分离, 可以分为一阶段模型和二阶段模型，one-stage 和 two-stage。\r\none-stage的代表作是YOLO，\r\ntwo-stage的代表作是Faster-RCNN","commend":0,"watch":0,"evaluate":0,"date":"2022-03-26T11:32:39.208Z"},{"title":"FCOS","img":"","href":"ai/ai-8/1","des":"# FCOS\r\n\r\n## 信息\r\n\r\n文章标题：FCOS: Fully Convolutional One-Stage Object Detection \r\n\r\n文章链接：[https://arxiv.org/pdf/1904.01355.pdf](https://arxiv.org/pdf/1904.01355.pdf)\r\n\r\n发表时间：2019-04\r\n\r\n\r\n## 背景\r\n\r\n\r\n## 创新点简介\r\n\r\n\r\n## 问题\r\n存在的问题，使用每一个预测框里的像素进行分类和预测，会导致背景参与计算，这样的运算是没有意义的。甚至是无效的。\r\n问题疑点：考虑到感受野的问题，肯能边上的特征像素也能够完成对物品框边缘的预测，尤其是边缘像素更有可能包含了物体边缘的信息，要小心处理\r\n\r\n\r\n## 引用","commend":0,"watch":0,"evaluate":0,"date":"2022-03-26T11:32:39.208Z"},{"title":"MEInst","img":"../../../img/article/2021-12-03-16-00-40.png","href":"ai/ai-8/2","des":"# MEInst\r\n\r\n## 信息\r\n\r\n文章标题：Mask Encoding for Single Shot Instance Segmentation\r\n\r\n文章链接：[https://arxiv.org/abs/2003.11712](https://arxiv.org/abs/2003.11712)\r\n\r\n发表时间：2020-03\r\n\r\n\r\n## 背景\r\n本文采用了紧凑掩膜编码，来进行单阶段实例分割任务，任务表面，使用了紧凑掩膜，对小目标的分割的效果相较二阶段无紧密掩膜的有较大提高，但对大目标，精度会少许的损失。\r\n\r\n## 创新点简介\r\n论文中指出，决定一个目标掩膜的关键像素主要分布在一个实例的边缘，而实例中占有大面积的部分是图像的主体部分，导致了信息的冗余。所以需要进行编码处理。\r\n> The discriminative pixels are mainly distributed along the object boundaries while most pixels in its body hold the properties of being category-c","commend":0,"watch":0,"evaluate":0,"date":"2022-03-26T11:32:39.208Z"},{"title":"CommunityLearning","img":"../../../img/article/2021-12-10-14-59-50.png","href":"ai/ai-5/3","des":"# CommunityLearning\r\n\r\n## 信息\r\n\r\n文章标题：Weakly Supervised Instance Segmentation by Deep Community Learning\r\n\r\n文章链接：[https://arxiv.org/pdf/2001.11207.pdf](https://arxiv.org/pdf/2001.11207.pdf)\r\n\r\n发表时间：2020-01\r\n\r\n\r\n## 背景\r\n\r\n\r\n## 创新点简介\r\n(社区学习， community learning)这篇文章通过训练多个子模型用于多个子任务，end-to-end trainable deep neural network with active interactions between multiple tasks。\r\n\r\n社区学习与多任务学习不同，后者试图在没有参与模块之间紧密互动的情况下平行实现多个目标。\r\nThe community learning is different from multi-task learning that attempts to achiev","commend":0,"watch":0,"evaluate":0,"date":"2022-03-26T11:32:39.207Z"},{"title":"BoxInst变量","img":"../../../img/article/2021-11-01-10-35-26.png","href":"ai/ai-5/4","des":"# BoxInst 变量\r\n本文简单介绍BoxInst 模型中所使用到的关键变量\r\n\r\n## 信息\r\n\r\n论文标题: BoxInst: High-Performance Instance Segmentation with Box Annotations\r\n\r\n论文链接：[https://openaccess.thecvf.com/content/CVPR2021/html/Tian_BoxInst_High-Performance_Instance_Segmentation_With_Box_Annotations_CVPR_2021_paper.html](https://openaccess.thecvf.com/content/CVPR2021/html/Tian_BoxInst_High-Performance_Instance_Segmentation_With_Box_Annotations_CVPR_2021_paper.html)\r\n\r\n发布时间: 2020-12 (CVPR 2021)\r\n\r\n## 创新点总结\r\n提出了一种高性能、仅使用边界框注释进行训练的任务级实例","commend":0,"watch":0,"evaluate":0,"date":"2022-03-26T11:32:39.207Z"},{"title":"ISTR","img":"../../../img/article/2022-02-26-14-06-17.png","href":"ai/ai-5/5","des":"# ISTR\r\n\r\n## 信息 \r\n文章链接：[https://arxiv.org/abs/2105.00637](https://arxiv.org/abs/2105.00637)\r\n\r\n发表时间：2021-05\r\n\r\n![](../../../img/article/2022-02-26-14-06-17.png)\r\n\r\n## 创新点简介\r\n本文设计了一款TransFormer结构，类比了Spacer RCNN的思想，使用固定数目的RoI对目标进行界框检测和实例分割。\r\n","commend":0,"watch":0,"evaluate":0,"date":"2022-03-26T11:32:39.207Z"}]}