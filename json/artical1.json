{"articals":[{"title":"DANet","img":"../../../img/artical/2022-02-18-17-29-10.png","href":"ai/ai-3/1","des":"# DANet \r\n\r\n## 信息 \r\n文章链接：[https://arxiv.org/abs/1809.02983](https://arxiv.org/abs/1809.02983)\r\n发表时间：2018-09\r\n\r\n\r\n## 创新点简介\r\n本文使用自注意力机制，在语义分割上加入了位置自注意力机制和通道自注意力机制，根据作者表述，位置自注意力机制将图片中相似的内容进行互相增强`【原文：any two positions with similar features can contribute mutual improvement regardless of their distance in spatial dimension】`，而通道自注意力机制则可以捕获任意通道之间的互相依赖关系。`【原文：we use the similar self-attention mechanism to capture the channel dependencies between any two channel maps, and update each channel map with a ","commend":0,"watch":0,"evaluate":0,"date":"2022-03-21T18:04:01.342Z"},{"title":"BoxInst变量","img":"","href":"ai/ai-3/2","des":"# BoxInst 变量\r\n本文简单介绍BoxInst 模型中所使用到的关键变量\r\n\r\n## 信息\r\n论文标题: BoxInst: High-Performance Instance Segmentation with Box Annotations\r\n发布时间: 2020-12\r\n\r\n## 创新点总结\r\n\r\n\r\n<pre><embed type=\"image/svg+xml\" src=\"../../../img/artical/boxinst_vars.svg\" /></pre>","commend":0,"watch":0,"evaluate":0,"date":"2022-03-21T17:46:10.987Z"},{"title":"coco数据集","img":"","href":"ai/ai-0/0","des":"# coco 数据集介绍\r\n\r\nMS COCO的全称是Microsoft Common Objects in Context ，COCO数据集是微软构建的一个数据集，其中包含丰富的物体检测，分割和关键点数据。这个数据集以场景理解为目标，与PASCAL VOC数据集相比，COCO中的数据集从复杂的日常场景中截取，背景更为复杂，目标数量比较多且目标尺寸更小，因此在COCO数据集上的实现好的效果更为困难。到目前为止，拥有最大的图像分割数据集，一共存在80个标注类别，有超过33 万张图片，其中20 万张有标注，整个数据集中个体的数目超过150 万个。","commend":0,"watch":0,"evaluate":0,"date":"2022-03-21T13:07:33.053Z"},{"title":"BlendMask","img":"../../../img/artical/2022-02-25-15-11-58.png","href":"ai/ai-2/0","des":"# BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation\r\n\r\n## 信息\r\n文章链接：[https://arxiv.org/abs/2001.0309](https://arxiv.org/abs/2001.0309)\r\n\r\n## 创新点简介\r\n本文使用提出了blender模型，将低层信息和高层信息融合，取得了更好的效果。其中高层信息会被制作成Attention map，而低层信息则包含更多的区分细节。轻量级模型在1080Ti上可达25FPS，用COCO数据集，mAP达34.2%。\r\n\r\n\r\n## 详细内容\r\n### 模型结构\r\n![](../../../img/artical/2022-02-25-15-11-58.png)\r\n上文所说的低层信息，就是这里从骨干和FPN输出中提取出来的`Bases`，而所谓高层特征则是通过了一个个又通过了Tower之后又经过`Boxes Attns`模块的信息，他们通过Blender进行相乘，然后相加融合，最终的输出。<br/>\r\n\r\n![](../../../img/artic","commend":0,"watch":0,"evaluate":0,"date":"2022-03-21T12:55:21.642Z"},{"title":"单组件开发","img":"","href":"vue/vue-0/1","des":"# 使用Vue 进行单组件开发 \r\n\r\n## 1. 安装vue-cli-service-global\r\n```shell \r\ncnpm install @vue/cli-service-global -g\r\ncnpm install vue-template-compiler -g\r\n```\r\n\r\n\r\n## 使用vue-cli-service 进行\r\n```shell \r\nvue-cli-service serve App.vue\r\n```\r\n\r\n\r\n## 参考资料\r\nhttps://www.cnblogs.com/Grani/p/14188144.html \r\n","commend":0,"watch":0,"evaluate":0,"date":"2022-03-20T15:43:00.390Z"},{"title":"nvm","img":"","href":"env/env-0/0","des":"\r\n\r\n## NVM 常见命令\r\n```shell\r\nnvm install stable ## 安装最新稳定版 node\r\nnvm install <version> ## 安装指定版本\r\nnvm uninstall <version> ## 删除已安装的指定版本\r\nnvm use <version> ## 切换使用指定的版本node\r\nnvm ls ## 列出所有安装的版本\r\nnvm ls-remote ## 列出所有远程服务器的版本\r\nnvm current ## 显示当前的版本\r\nnvm alias <name> <version> ## 给不同的版本号添加别名\r\nnvm unalias <name> ## 删除已定义的别名\r\nnvm reinstall-packages <version> ## 在当前版本 node 环境下，重新全局安装指定版本号的 npm 包\r\nnvm alias default [node版本号] ##设置默认版本\r\n```\r\n\r\n\r\n## npm 查看所有全局\r\n```shell\r\ncnpm list -g --depth 0\r\n```","commend":0,"watch":0,"evaluate":0,"date":"2022-03-20T11:46:55.032Z"},{"title":"Vue安装","img":"","href":"vue/vue-0/0","des":"# Vue 环境配置 \r\n\r\n\r\n## 使用 nvm 创建合适的npm\r\n```shell\r\nnvm install <version>\r\n```\r\n\r\n## 安装cnpm\r\n```shell\r\nnpm install cnpm -g\r\n```\r\n\r\n## 安装vue相关内容 \r\n```shell\r\ncnpm install -g @vue/cli\r\n```\r\n\r\n","commend":0,"watch":0,"evaluate":0,"date":"2022-03-20T11:46:53.662Z"},{"title":"sparse_RCNN","img":"../../../img/artical/2022-03-15-15-25-43.png","href":"ai/ai-6/0","des":"# Sparse RCNN\r\n\r\n## 信息\r\n\r\n论文题目：Sparse R-CNN: End-to-End Object Detection with Learnable Proposals\r\n\r\n论文链接：[https://arxiv.org/abs/2106.02351](https://arxiv.org/abs/2106.02351)\r\n\r\n## 创新\r\n\r\n使用可学习的100-300个边框来取代RPN（区域建议网络），实现了一个完全稀疏的端到端目标检测网络。\r\n\r\n\r\n## 详情\r\n\r\n### 稀疏和密集\r\n![](../../../img/artical/2022-03-15-15-25-43.png)\r\n作者指出以前的目标检测都是每个特征像素做的，采用了“anchor boxes”机制，这会有十分密集的目标框（HWk个）产生，Faster RCNN 使用NMS筛选值的计算分类和边框回归的建议框，让一个密集的检测变得稀疏起来，但他仍然不能算是一个完全稀疏的目标检测方法（图b）。而本文作者所提出的Sparse RCNN则使用学习来的N(100或300)个RPN（可以理解为","commend":0,"watch":0,"evaluate":0,"date":"2022-03-15T11:03:09.150Z"},{"title":"SOLQ","img":"../../../img/artical/2022-03-12-13-59-28.png","href":"ai/ai-5/2","des":"# SOLQ\r\n\r\n## 信息\r\n\r\n论文题目：SOLQ: Segmenting Objects by Learning Queries\r\n\r\n论文链接：[https://arxiv.org/abs/2106.02351](https://arxiv.org/abs/2106.02351)\r\n\r\n## 创新\r\n\r\nSOLQ基于近期所提出的 DETR的实例分割的端到端框架，通过学习统一的查询来分割目标。不同于DETR通过引入类似于MaskRCNN中的Mask分支完成分割，SOLQ中的每个查询代表一个对象，里面包含了所有的class, location 和 mask信息。\r\n\r\n![](../../../img/artical/2022-03-12-13-59-28.png)\r\n\r\n如上图所示，对于DETR，它通过设置一个长采样卷积结构完成对于实例mask的获取\r\n\r\n![](../../../img/artical/2022-03-12-13-58-55.png)\r\n![](../../../img/artical/2022-03-12-14-03-40.png)\r\n\r\n而对于SOLQ ","commend":0,"watch":0,"evaluate":0,"date":"2022-03-12T10:19:24.968Z"},{"title":"SETR","img":"../../../img/artical/2022-03-11-14-59-06.png","href":"ai/ai-4/0","des":"# SETR\r\n\r\n## 信息\r\n\r\n论文题目：Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers\r\n\r\n文章链接：\r\n\r\n## 创新点简介\r\nSETR使用transformer设计了一个端到端的语义分割网络，首先将原图切割为若干 16x16 个窗口，把其中的像素进行线性映射，得到一维编码，然后使用24层transformer的编码器来完成对于图像特征的提取，然后使用卷积做上采样操作，得到最终结果。\r\n\r\n![](../../../img/artical/2022-03-11-14-59-06.png)\r\n\r\n## 优点\r\n是语义分割领域的一次创行，将transformer引入到了语义分割领域中。\r\n\r\n## 存在的问题\r\n切割的窗口过大，语义信息不仅准。\r\n","commend":0,"watch":0,"evaluate":0,"date":"2022-03-12T10:19:24.959Z"}]}